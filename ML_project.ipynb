{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f3aeb1-7ef0-488a-a91f-57c9f1461e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ad14a9-cc80-40fa-be60-c46718803923",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/MetroPT3(AirCompressor).csv', index_col = False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db36fe69",
   "metadata": {},
   "source": [
    "# 1. Data cleaning and preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242d02e6",
   "metadata": {},
   "source": [
    "According to the documentation, the following preprocessing steps have been conducted before publishing the data:\n",
    "\n",
    "- Data segmentation\n",
    "- Normalization\n",
    "- Feature Extraction\n",
    "\n",
    "Thus, we do not need to apply them in our work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39eac0f",
   "metadata": {},
   "source": [
    "### 1) Overview "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a0f6d3-6506-4d21-ad5e-038457ada2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of null values: {data.isna().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a10084d-a437-48db-9147-3d799038f9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of duplicates: {data.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986aa2c-f6c6-4ba0-8af1-d541e0ccb6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfb9f93",
   "metadata": {},
   "source": [
    "### 2) drop unnecessary columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ff329-4364-40ea-a3a7-1ac7665a5106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unecessary columns\n",
    "data.drop(['Unnamed: 0'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849b9d39",
   "metadata": {},
   "source": [
    "### 3) Add a label Column \n",
    "From the failure information table provided int the data description file below, we will try to label the data and evaluate the effectiveness of failure prediction algorithms: \n",
    "\n",
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902e9b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = data.copy()\n",
    "labeled_data['status'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6092ea",
   "metadata": {},
   "source": [
    "#### Converting the timestamp column into pandas.DateTime data type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95922fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the timestamp to datetime\n",
    "labeled_data['timestamp'] = pd.to_datetime(labeled_data['timestamp'], format = '%Y-%m-%d %H:%M:%S')\n",
    "print(\"current data type of timestamp: \", labeled_data['timestamp'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14cb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to convert time to pandas.datetime \n",
    "def convert_time(X):\n",
    "    result =[]\n",
    "    for x in X:\n",
    "        result.append(pd.to_datetime(x, format = '%Y-%m-%d %H:%M:%S'))\n",
    "    return result\n",
    "\n",
    "failure_start_time = convert_time([\"2020-04-18 00:00:00\", \"2020-05-29 23:30:00\", \"2020-06-05 10:00:00\", \"2020-07-15 14:30:00\"])\n",
    "failure_end_time = convert_time([\"2020-04-18 23:59:00\", \"2020-05-30 06:00:00\", \"2020-06-07 14:30:00\", \"2020-07-15 19:00:00\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e39fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through the data and label the data\n",
    "for start, end in zip(failure_start_time, failure_end_time):\n",
    "    labeled_data.loc[(labeled_data['timestamp'] >= start) & (labeled_data['timestamp'] <= end), 'status'] = 1\n",
    "    #check if any failures were missed or\n",
    "    print(f\"number of failures between {start} and {end}: {labeled_data.loc[(labeled_data['timestamp'] >= start) & (labeled_data['timestamp'] <= end), 'status'].sum()}\")\n",
    "    \n",
    "print(f\"number of failures: {labeled_data['status'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af245553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for positive class imbalance\n",
    "print(f\"Example of Failure state \\n {labeled_data[labeled_data['status']==1].head()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd1448",
   "metadata": {},
   "source": [
    "### 4)Subsampling the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463666b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into positive and negative samples \n",
    "positive_samples = labeled_data[labeled_data['status'] == 1]\n",
    "negative_samples = labeled_data[labeled_data['status'] == 0]\n",
    "\n",
    "#print the shape of the positive and negative samples\n",
    "print(f\"shape of positive samples: {positive_samples.shape}\")\n",
    "print(f\"shape of negative samples: {negative_samples.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8b36a5",
   "metadata": {},
   "source": [
    "There is around 30k positive samples and  1500k negative samples. This indicates a highly imbalanced  dataset, which can be challenging to handle. Thus, we need to subsample the negative class to balance our data. \n",
    "\n",
    "In order to do so, we will randomly sample 30k negative samples from the 1500k sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf36e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsample the negative class to balance the data\n",
    "negative_samples = negative_samples.sample(n = positive_samples.shape[0], random_state = 42)\n",
    "print(f\"Negative dataset after subsampling {negative_samples.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b787c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot pie chart to show the class distribution\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.pie([positive_samples.shape[0], negative_samples.shape[0]], labels = ['Positive', 'Negative'], autopct = '%1.1f%%', startangle = 90, colors = ['lightpink', 'lightblue'])\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffc793c",
   "metadata": {},
   "source": [
    "Now, we merge both the positive and negative samples into a single set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a82fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the positive and negative samples\n",
    "merged_data = pd.concat([positive_samples, negative_samples], axis = 0)\n",
    "print(f\"shape of merged data: {merged_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3777688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2d8e03",
   "metadata": {},
   "source": [
    "### 5) Checking for outliers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb42acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outliers(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    num_outliers = len(outliers)\n",
    "    print(f\"Number of outliers in {column}: {num_outliers}\")\n",
    "    return outliers\n",
    "\n",
    "def remove_outliers(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers_removed = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n",
    "    num_removed = len(data) - len(outliers_removed)\n",
    "    print(f\"Number of outliers removed from {column}: {num_removed}\\n\")\n",
    "    return outliers_removed\n",
    "\n",
    "# First, identify outliers\n",
    "clean_data = merged_data.copy()\n",
    "for col in clean_data:\n",
    "    if col not in ['timestamp', 'status']:\n",
    "        outliers = identify_outliers(clean_data, col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e786b310",
   "metadata": {},
   "source": [
    "the features: ['COMP', 'DV_eletric','Towers', 'MPG','LPS','Pressure_switch','Oil_level','Caudal_impulses'] are binary features. So we do not remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4587e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in clean_data:\n",
    "    if col not in ['timestamp', 'status', 'LPS', 'Pressure_switch', 'Oil_level', 'Caudal_impulses']:\n",
    "        cleaned_data = remove_outliers(clean_data, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356346ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate the columns with the binary values\n",
    "binary_cols = ['LPS', 'Pressure_switch', 'Oil_level', 'Caudal_impulses']\n",
    "#Ensure the the binary data is binary\n",
    "cleaned_data[binary_cols] = cleaned_data[binary_cols].apply(np.round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of unique values in each column\n",
    "for col in cleaned_data.columns:\n",
    "    print(f\"number of unique values in {col}: {cleaned_data[col].nunique()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd56659",
   "metadata": {},
   "source": [
    "# 2. Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3b218a",
   "metadata": {},
   "source": [
    "### 1) Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation \n",
    "correlation = cleaned_data.corr()\n",
    "plt.figure(figsize = (10, 10))\n",
    "sns.heatmap(correlation, annot = True, cmap = 'coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe5a44",
   "metadata": {},
   "source": [
    "From the above correlation heatmap,  we can see that our target feature **\"status\"** has a strong correlation with these features: TP2, H1, DV_pressure, Oil_temparature, Motor_current, COMP, DV_electric and MPG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b845db8",
   "metadata": {},
   "source": [
    "### 2) Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5724c60",
   "metadata": {},
   "source": [
    "1. Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44513f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize all the features outliers in one plot \n",
    "sns.set(rc={'figure.figsize':(20,8.27)})\n",
    "sns.boxplot(data = cleaned_data.drop(['timestamp', 'status'], axis = 1))\n",
    "# plt.xticks(rotation = 45)\n",
    "plt.title('Boxplot of all features')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee1ba6",
   "metadata": {},
   "source": [
    "2. Probability distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0acb70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the probability distribution of all the features\n",
    "def plot_col_distribution(data):\n",
    "    fig, axes = plt.subplots(4, 4, figsize = (20, 10))\n",
    "    axes = axes.flatten()\n",
    "    for i, col in enumerate(data.columns):\n",
    "        sns.distplot(data[col], ax = axes[i])\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_col_distribution(cleaned_data.drop(['timestamp', 'status'], axis = 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b83150",
   "metadata": {},
   "source": [
    "3. Time series plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35580505",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.iloc[:,:16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec9963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorganize according to timestamp \n",
    "cleaned_data.sort_values('timestamp', inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb99ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series\n",
    "cleaned_data.iloc[:,:16].plot(\n",
    "        subplots =True,\n",
    "        layout=(6, 3),\n",
    "        figsize=(22,22),\n",
    "        fontsize=10, \n",
    "        linewidth=1,\n",
    "        sharex = False, \n",
    "        title='Visualization of the Original Time Series')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
