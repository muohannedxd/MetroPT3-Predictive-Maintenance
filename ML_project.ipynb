{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f3aeb1-7ef0-488a-a91f-57c9f1461e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import random\n",
    "# from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ad14a9-cc80-40fa-be60-c46718803923",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/MetroPT3(AirCompressor).csv', index_col = False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db36fe69",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning and Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242d02e6",
   "metadata": {},
   "source": [
    "According to the documentation, the following preprocessing steps have been conducted before publishing the data:\n",
    "\n",
    "- Data segmentation\n",
    "- Normalization\n",
    "- Feature Extraction\n",
    "\n",
    "Thus, we do not need to apply them in our work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39eac0f",
   "metadata": {},
   "source": [
    "### 1) Overview "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a0f6d3-6506-4d21-ad5e-038457ada2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of null values: {data.isna().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a10084d-a437-48db-9147-3d799038f9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of duplicates: {data.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986aa2c-f6c6-4ba0-8af1-d541e0ccb6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfb9f93",
   "metadata": {},
   "source": [
    "### 2) drop unnecessary columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ff329-4364-40ea-a3a7-1ac7665a5106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unecessary columns\n",
    "data.drop(['Unnamed: 0'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849b9d39",
   "metadata": {},
   "source": [
    "### 3) Add a label Column \n",
    "From the failure information table provided int the data description file below, we will try to label the data and evaluate the effectiveness of failure prediction algorithms: \n",
    "\n",
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902e9b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = data.copy()\n",
    "labeled_data['status'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6092ea",
   "metadata": {},
   "source": [
    "#### Converting the timestamp column into pandas.DateTime data type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95922fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the timestamp to datetime\n",
    "labeled_data['timestamp'] = pd.to_datetime(labeled_data['timestamp'], format = '%Y-%m-%d %H:%M:%S')\n",
    "print(\"current data type of timestamp: \", labeled_data['timestamp'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14cb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to convert time to pandas.dateTime \n",
    "def convert_time(X):\n",
    "    result =[]\n",
    "    for x in X:\n",
    "        result.append(pd.to_datetime(x, format = '%Y-%m-%d %H:%M:%S'))\n",
    "    return result\n",
    "\n",
    "failure_start_time = convert_time([\"2020-04-18 00:00:00\", \"2020-05-29 23:30:00\", \"2020-06-05 10:00:00\", \"2020-07-15 14:30:00\"])\n",
    "failure_end_time = convert_time([\"2020-04-18 23:59:00\", \"2020-05-30 06:00:00\", \"2020-06-07 14:30:00\", \"2020-07-15 19:00:00\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e39fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through the data and label the data\n",
    "for start, end in zip(failure_start_time, failure_end_time):\n",
    "    labeled_data.loc[(labeled_data['timestamp'] >= start) & (labeled_data['timestamp'] <= end), 'status'] = 1\n",
    "    #check if any failures were missed or\n",
    "    print(f\"number of failures between {start} and {end}: {labeled_data.loc[(labeled_data['timestamp'] >= start) & (labeled_data['timestamp'] <= end), 'status'].sum()}\")\n",
    "    \n",
    "print(f\"number of failures: {labeled_data['status'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af245553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for positive class imbalance\n",
    "print(f\"Example of Failure state \\n {labeled_data[labeled_data['status']==1].head()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e2931a-00ae-479d-a2c5-1cc6ee1ea3dc",
   "metadata": {},
   "source": [
    "### 4) Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aeed59-07d3-4bae-8178-f574acaa7564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labeled_data.sort_values('timestamp', inplace = True)\n",
    "y = labeled_data.status\n",
    "X = labeled_data.copy().drop(columns=['status'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                   random_state=42, \n",
    "                                   test_size=0.25, \n",
    "                                   shuffle=True)\n",
    "\n",
    "print(f'shape of X_train: {X_train.shape}')\n",
    "print(f'shape of X_test: {X_test.shape}')\n",
    "print(f'shape of y_train: {y_train.shape}')\n",
    "print(f'shape of y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd1448",
   "metadata": {},
   "source": [
    "### 5) Balancing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1a1e02-6c19-49d6-9600-025c6ad3c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2923e0a5-089e-4b3f-9619-8b379851eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0e944e-24a1-4073-828f-07c463a0fe34",
   "metadata": {},
   "source": [
    "The number of negative values (normal cases) is way too large compared to the positive class (around 22k positive samples and 1100k negative samples). Then, we are running into an Imbalaned Dataset. It is expected since we are dealing with a predictive maintenance problem.  \n",
    "To address this issue, we ought to balance our data. There are various techniques to balance it. Here is a overview about some of them:  \n",
    "* **Undersampling:** reduces the number of instances in the majority class to match the number of minority class instances by randomly selecting them. It is the fastest and most intuitive technique.\n",
    "* **Oversampling:** increases the number of instances of the minority class by replicating or generating new instances.  \n",
    "* **SMOTE (Synthetic Minority Oversampling Technique):** generates synthetic instances by interpolating between existing instances in the minority class.  \n",
    "* **SMOTE Tomek:** identifies Tomek links (pairs of instances from different classes) and removes majority class instances from the pairs while oversampling the minority class using SMOTE. This technique takes significantly more time than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5156dd3-ff17-4366-a5f9-66144f81bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "# ros = RandomOverSampler(random_state=42)\n",
    "# smote = SMOTE(random_state=42)\n",
    "# smote_tomek = SMOTETomek(random_state=42)\n",
    "\n",
    "X_train_balanced, y_train_balanced = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# reassemble the dataset for further preprocessing and EDA\n",
    "balanced_train = X_train_balanced.copy()\n",
    "balanced_train['status'] = y_train_balanced\n",
    "balanced_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75059c6b-7e85-47a0-9845-ae3be23250ee",
   "metadata": {},
   "source": [
    "`balanced_train` is supposed to be balanced now. Let us check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b1a540-2c27-4532-928c-9aa9f1955795",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_balanced.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b787c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value counts from the imbalanced dataset\n",
    "imbalanced_class_counts = y_train.value_counts()\n",
    "\n",
    "# value counts from the balanced dataset\n",
    "balanced_class_counts = y_train_balanced.value_counts()\n",
    "\n",
    "# plot pie charts to show the class distribution difference\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].pie(\n",
    "    imbalanced_class_counts,\n",
    "    labels = ['Negative', 'Positive'],\n",
    "    autopct = '%1.1f%%',\n",
    "    startangle = 90,\n",
    "    colors = ['lightpink', 'lightblue']\n",
    ")\n",
    "axes[0].set_title('Before Undersampling')\n",
    "\n",
    "axes[1].pie(\n",
    "    balanced_class_counts,\n",
    "    labels = ['Negative', 'Positive'],\n",
    "    autopct = '%1.1f%%',\n",
    "    startangle = 90,\n",
    "    colors = ['lightpink', 'lightblue']\n",
    ")\n",
    "axes[1].set_title('After Undersampling')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3777688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2d8e03",
   "metadata": {},
   "source": [
    "### 6) Checking for outliers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb42acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outliers(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    num_outliers = len(outliers)\n",
    "    print(f\"Number of outliers in {column}: {num_outliers}\")\n",
    "    return outliers\n",
    "\n",
    "# def remove_outliers(data, column):\n",
    "#     Q1 = data[column].quantile(0.25)\n",
    "#     Q3 = data[column].quantile(0.75)\n",
    "#     IQR = Q3 - Q1\n",
    "#     lower_bound = Q1 - 1.5 * IQR\n",
    "#     upper_bound = Q3 + 1.5 * IQR\n",
    "#     outliers_removed = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n",
    "#     num_removed = len(data) - len(outliers_removed)\n",
    "#     print(f\"Number of outliers removed from {column}: {num_removed}\\n\")\n",
    "#     return outliers_removed\n",
    "\n",
    "# First, identify outliers\n",
    "for col in balanced_train:\n",
    "    if col not in ['timestamp', 'status']:\n",
    "        outliers = identify_outliers(balanced_train, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e786b310",
   "metadata": {},
   "source": [
    "the features: ['COMP', 'DV_eletric','Towers', 'MPG','LPS','Pressure_switch','Oil_level','Caudal_impulses'] are binary features. So we do not remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4587e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the columns with the binary values\n",
    "binary_cols = ['LPS', 'Pressure_switch', 'Oil_level', 'Caudal_impulses']\n",
    "# Ensure the the binary data is binary\n",
    "balanced_train[binary_cols] = balanced_train[binary_cols].apply(np.round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356346ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of unique values in each column\n",
    "for col in balanced_train.columns:\n",
    "    print(f\"number of unique values in {col}: {balanced_train[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd56659",
   "metadata": {},
   "source": [
    "# 2. Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3b218a",
   "metadata": {},
   "source": [
    "### 1) Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation \n",
    "correlation = balanced_train.corr()\n",
    "plt.figure(figsize = (18, 10))\n",
    "sns.heatmap(correlation, annot = True, fmt='.2f', cmap = 'coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe5a44",
   "metadata": {},
   "source": [
    "From the above correlation heatmap,  we can see that our target feature **\"status\"** has a strong correlation with these features: TP2, H1, DV_pressure, Oil_temparature, Motor_current, COMP, DV_electric and MPG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b845db8",
   "metadata": {},
   "source": [
    "### 2) Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5724c60",
   "metadata": {},
   "source": [
    "1. Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44513f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize all the features outliers in one plot \n",
    "sns.set(rc={'figure.figsize':(20,8.27)})\n",
    "# sns.boxplot(data = balanced_train.drop(['timestamp', 'status'], axis = 1))\n",
    "sns.boxplot(data = balanced_train.drop(['status'], axis = 1))\n",
    "# plt.xticks(rotation = 45)\n",
    "plt.title('Boxplot of all features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee1ba6",
   "metadata": {},
   "source": [
    "2. Probability distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0acb70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module='seaborn')\n",
    "\n",
    "#visualize the probability distribution of all the features\n",
    "def plot_col_distribution(data):\n",
    "    fig, axes = plt.subplots(4, 4, figsize = (20, 10))\n",
    "    axes = axes.flatten()\n",
    "    for i, col in enumerate(data.columns):\n",
    "        data[col] = data[col].replace([np.inf, -np.inf], np.nan)\n",
    "        sns.histplot(data[col], ax = axes[i], kde=True)\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_col_distribution(balanced_train.drop(['timestamp', 'status'], axis = 1))\n",
    "# plot_col_distribution(balanced_train.drop(['status'], axis = 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b83150",
   "metadata": {},
   "source": [
    "3. Time series plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35580505",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_train.iloc[:,:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec9963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorganize according to timestamp \n",
    "balanced_train.sort_values('timestamp', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb99ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series\n",
    "balanced_train.iloc[:,:16].plot(\n",
    "        subplots =True,\n",
    "        layout=(6, 3),\n",
    "        figsize=(22,22),\n",
    "        fontsize=10, \n",
    "        linewidth=1,\n",
    "        sharex = False, \n",
    "        title='Visualization of the Original Time Series')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767d78cf-7d8a-44d9-9038-8c9e0202ffcf",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97535f8e-74ff-45e2-bb60-92ae902a4a4e",
   "metadata": {},
   "source": [
    "#### **Taking out the Timestamp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e376c7-68c7-4e17-9f39-4a362f35831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_balanced = X_train_balanced.drop(columns=['timestamp'])\n",
    "X_test = X_test.drop(columns=['timestamp'])\n",
    "X_train = X_train.drop(columns=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2014b265-a530-47a8-9310-8a3e6eb9580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_balanced.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8607e8",
   "metadata": {},
   "source": [
    "#### **Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422b29f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_train_balanced.columns)\n",
    "\n",
    "X_train_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea2e3cc-bf6e-429d-8a4d-58b8f5e69399",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b44ebd-67ce-410a-be5b-8005081723c2",
   "metadata": {},
   "source": [
    "#### **Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57de174-f548-433b-b16e-2ffc00cd87ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# univariate features selection\n",
    "k = 7  # Adjust this value as needed\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X_selected= selector.fit_transform(X_train_balanced, y_train_balanced)\n",
    "\n",
    "features = X_train_balanced.columns\n",
    "\n",
    "# Create a DataFrame with feature names and their F-statistic scores\n",
    "feature_scores = pd.DataFrame({'Feature': features, 'F-Score': selector.scores_})\n",
    "feature_scores = feature_scores.sort_values(by='F-Score', ascending=False)\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='F-Score', y='Feature', data=feature_scores, palette='viridis')\n",
    "plt.title('Feature Importance (F-Scores)')\n",
    "plt.xlabel('F-Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8d3a82",
   "metadata": {},
   "source": [
    "#### **Removing highly correlated features (TP3, TP2, H1 and MPG)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b89eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_remove = ['TP3', 'TP2', 'MPG', 'Pressure_switch', 'LPS']\n",
    "\n",
    "# Remove the specified features from the DataFrame\n",
    "X_test.drop(features_to_remove, axis=1, inplace=True)\n",
    "X_train_balanced.drop(features_to_remove, axis=1, inplace=True)\n",
    "X_train.drop(features_to_remove, axis=1, inplace=True)\n",
    "X_test_scaled.drop(features_to_remove, axis=1, inplace=True)\n",
    "X_train_scaled.drop(features_to_remove, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "X_test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb43c5e-878e-46c0-8fa9-8ef426b25ae5",
   "metadata": {},
   "source": [
    "#### **Evaluation Protocol**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e10ff2-e055-4e90-b86d-e236d937a73e",
   "metadata": {},
   "source": [
    "Reminder that the aim of this study is to predict failures and the need of maintenance in an urban metro public transportation service. To assess the fit and how good each model performed, we will evaluate it using the following metrics:\n",
    "\n",
    "* **Accuracy:** measures the overall correctness of the model since our data is balanced:\n",
    "\n",
    "$$ Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "\n",
    "* **Precision:** the goal of it is to maximize the positive (failure) predictions when they were originally failures (TP) and minimize false calls (FP) which are non-failure values predicted as failure by the model:\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "* **F1 Score**: harmonic mean of Precision and Recall:\n",
    "\n",
    "$$ F1 Score = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} $$\n",
    "\n",
    "However, we will mostly focus on the $F1$ and $Precision$ scores to evaluate and validate our models and to deduce what is the model that best fits our data and generalizes to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d8320c-4455-4dfb-94c5-30321017b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4d7cb0-4cfa-422b-a69b-83dd3c526abd",
   "metadata": {},
   "source": [
    "We maintain a dataframe `scores` to hold the scores of each model we will study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bed47f-5bec-4ed0-9427-db5dee9d8c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(columns=['model', 'accuracy', 'precision', 'f1'])\n",
    "scores # should be empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56743e17-6e81-4874-beac-64df61126971",
   "metadata": {},
   "source": [
    "### 1) Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e815f9-d4d6-439d-8fde-9c664decfab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "dt_preds = dt.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "train_accuracy_dt = accuracy_score(y_train, dt.predict(X_train))\n",
    "test_accuracy_dt = accuracy_score(y_test, dt_preds)\n",
    "\n",
    "# precision\n",
    "train_precision_dt = precision_score(y_train, dt.predict(X_train))\n",
    "test_precision_dt = precision_score(y_test, dt_preds)\n",
    "\n",
    "# f1 score\n",
    "train_f1_dt = f1_score(y_train, dt.predict(X_train))\n",
    "test_f1_dt = f1_score(y_test, dt_preds)\n",
    "\n",
    "print(f'accuracy:')\n",
    "print(f'  train: {train_accuracy_dt}')\n",
    "print(f'  test: {test_accuracy_dt}')\n",
    "\n",
    "print(f'\\nprecision:')\n",
    "print(f'  train: {train_precision_dt}')\n",
    "print(f'  test: {test_precision_dt}')\n",
    "\n",
    "print(f'\\nf1:')\n",
    "print(f'  train: {train_f1_dt}')\n",
    "print(f'  test: {test_f1_dt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c22b71-f5ff-4bc3-aea8-b8917dfce735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    dt, X_train, y_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=5, scoring='f1'\n",
    ")\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Training F1 Score')\n",
    "plt.plot(train_sizes, np.mean(test_scores, axis=1), 'o-', label='Validation F1 Score')\n",
    "plt.xlabel('Dataset Size')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Learning Curve of Decision Tree')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74684692-3e3a-4c63-a05a-f826f7f88799",
   "metadata": {},
   "source": [
    "The naive Decision Tree solution performed great. However, we believe that we can still get better results for the precision and f1 scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03596e4-4854-4b98-b4cc-7123c48a7995",
   "metadata": {},
   "source": [
    "Here, we are going to find the best Decision Tree Classifier. That is, we aim to find its parameters that best maximize the score. This is done by **Random Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7e09ff-7fe9-4a00-aef6-bc8ba4c63b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "dt_params = {\n",
    "    'criterion': ['gini'],\n",
    "    'max_depth': randint(1, 50),\n",
    "    'min_samples_split': randint(2, 11),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_dt = RandomizedSearchCV(\n",
    "    estimator=dt,\n",
    "    param_distributions=dt_params,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    n_jobs=1,\n",
    "    random_state=42,\n",
    "    scoring='f1'\n",
    ")\n",
    "\n",
    "random_search_dt.fit(X_train, y_train)\n",
    "\n",
    "best_params_dt = random_search_dt.best_params_\n",
    "best_score_dt = random_search_dt.best_score_\n",
    "\n",
    "print(f'best decision tree parameters: {best_params_dt}')\n",
    "print(f'best score (F1): {best_score_dt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e87540-abb9-4093-b977-257d943cef0f",
   "metadata": {},
   "source": [
    "Now, we model with the found as best **Decision Tree** with the **Random Search**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e64d0ca-25de-4a95-ae8d-9733d2d73e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dt = random_search_dt.best_estimator_\n",
    "best_dt.fit(X_train, y_train)\n",
    "\n",
    "best_dt_preds = best_dt.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "train_accuracy_best_dt = accuracy_score(y_train, best_dt.predict(X_train))\n",
    "test_accuracy_best_dt = accuracy_score(y_test, best_dt_preds)\n",
    "\n",
    "# precision\n",
    "train_precision_best_dt = precision_score(y_train, best_dt.predict(X_train))\n",
    "test_precision_best_dt = precision_score(y_test, best_dt_preds)\n",
    "\n",
    "# f1 score\n",
    "train_f1_best_dt = f1_score(y_train, best_dt.predict(X_train))\n",
    "test_f1_best_dt = f1_score(y_test, best_dt_preds)\n",
    "\n",
    "print(f'accuracy:')\n",
    "print(f'  train: {train_accuracy_best_dt}')\n",
    "print(f'  test: {test_accuracy_best_dt}')\n",
    "\n",
    "print(f'\\nprecision:')\n",
    "print(f'  train: {train_precision_best_dt}')\n",
    "print(f'  test: {test_precision_best_dt}')\n",
    "\n",
    "print(f'\\nf1:')\n",
    "print(f'  train: {train_f1_best_dt}')\n",
    "print(f'  test: {test_f1_best_dt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162d7838-93bb-43f9-a30d-38a00ae5e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "print(classification_report(y_test, best_dt_preds))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_test, best_dt_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_dt.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5dac8e-d523-4777-af2e-966594ffd8c9",
   "metadata": {},
   "source": [
    "We can directky notice that the model is able to capture many $True Positives$ data instances. Which is quite the desirable behaviour. To detect the $TPs$ (positive class and predicted as positive, aka: anomaly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1db338-6102-4ab0-9285-d994f8fbc5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to scores dataframe\n",
    "scores.loc[len(scores)] = ['Decision Tree', test_accuracy_best_dt, test_precision_best_dt, test_f1_best_dt]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46f0b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_dt, 'saved_models/decision_tree_model.joblib')\n",
    "df_pred = pd.DataFrame({'Prediction': best_dt_preds})\n",
    "df_pred.to_csv(\"predictions/decision_tree_model.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621df7f7-ec02-4baf-bb8d-41f214884dd1",
   "metadata": {},
   "source": [
    "### 2) Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3eab44-d260-4799-9e16-b9a141873a40",
   "metadata": {},
   "source": [
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. Trees in the forest use the best split strategy, i.e. equivalent to passing `splitter=\"best\"` to the underlying DecisionTreeRegressor. The sub-sample size is controlled with the `max_samples` parameter if `bootstrap=True` (default), otherwise the whole dataset is used to build each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be76daf-a2ea-4017-9117-e0f415c66f7a",
   "metadata": {},
   "source": [
    "**building a failure prediction model**\n",
    "\n",
    "The goal is to employ the random forest classifier algorithm in order to predict failure labeled as 0 and 1 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827d0281-2772-4264-8200-666dce538a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "rf_preds = rf.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "train_accuracy_rf = accuracy_score(y_train, rf.predict(X_train))\n",
    "test_accuracy_rf = accuracy_score(y_test, rf_preds)\n",
    "\n",
    "# precision\n",
    "train_precision_rf = precision_score(y_train, rf.predict(X_train))\n",
    "test_precision_rf = precision_score(y_test, rf_preds)\n",
    "\n",
    "# f1 score\n",
    "train_f1_rf = f1_score(y_train, rf.predict(X_train))\n",
    "test_f1_rf = f1_score(y_test, rf_preds)\n",
    "\n",
    "print(f'accuracy:')\n",
    "print(f'train: {train_accuracy_rf}')\n",
    "print(f'test: {test_accuracy_rf}')\n",
    "\n",
    "print(f'\\nprecision:')\n",
    "print(f'train: {train_precision_rf}')\n",
    "print(f'test: {test_precision_rf}')\n",
    "\n",
    "print(f'\\nf1:')\n",
    "print(f'train : {train_f1_rf}')\n",
    "print(f'test: {test_f1_rf}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7be98-98cb-4307-80fa-63ad51803226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "print(classification_report(y_test, rf_preds))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_test, rf_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21f63c7-9fd2-466d-b419-ecda8b67aa1e",
   "metadata": {},
   "source": [
    "**hyperparameters of Random Forest** \n",
    "\n",
    "Random forest has several hyperparameters that influence its performance: \n",
    "- `n_estimators`: The number of trees in the forest. Increasing this value generally improves performance until a certain point\n",
    "\n",
    "- ` max_depth`: The maximum depth of each tree. Deeper trees may capture more complex relationships but this may lead to an overfitting\n",
    "\n",
    "- `min_samples_split`: The minimum number of samples required to split an internal node.\n",
    "\n",
    "- `min_samples_leaf`:  The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least `min_samples_leaf` training samples in each of the left and right branches. This may have the effect of smoothing the model\n",
    "\n",
    "-  `max_features`: The maximum number of features to consider when looking for the best split.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ed9cda-cb86-4c72-968f-4d343137fe2d",
   "metadata": {},
   "source": [
    "We will fine-tune the model to achieve the most optimal performance by adjusting these hyperparametrs. Grid search and Cross-Validation common techniques for finding the best combination of hyperparameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789d65a4-d229-4fdb-8e38-81fcd43292c6",
   "metadata": {},
   "source": [
    "**Random search** is another powerful technique for optimizing the hyperparameters of a model. It works in a similar way to grid search cross-validation (GridSearchCV), but instead of searching over a predefined grid of hyperparameters, it samples them randomly from a distribution,  without becoming too computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a515a86c-fe9e-4bda-974d-4f5b4fc5f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized Search CV\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "# Perform Randomized Search CV\n",
    "Random_search = RandomizedSearchCV(rf, param_dist, cv=5, n_iter=10, n_jobs=-1, random_state=42)\n",
    "Random_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Extract the cross-validation results and hyperparameters\n",
    "cv_results = Random_search.cv_results_['mean_test_score']\n",
    "best_params = Random_search.cv_results_['params']\n",
    "\n",
    "# Get the best params \n",
    "best_params = Random_search.best_params_\n",
    "\n",
    "# train the model with the best params\n",
    "rf_best = Random_search.best_estimator_\n",
    "rf_best.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best Hyperparameters:')\n",
    "print(best_params)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_rf = rf_best.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "precision = precision_score(y_test, y_pred_rf)\n",
    "f1 = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"Random Forest Classifier:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Display confusion matrix\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "disp_forest = ConfusionMatrixDisplay(conf_matrix_rf, display_labels=rf_best.classes_)\n",
    "disp_forest.plot(cmap='Blues', values_format='d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed03240-a984-413c-9de2-b614fd9a89f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to scores dataframe\n",
    "scores_rf_best = {\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'f1': f1\n",
    "}\n",
    "\n",
    "scores_rf_best_df = pd.DataFrame(scores_rf_best, index=[0])\n",
    "scores.loc[len(scores)] = ['Best Random Forest', accuracy, precision, f1]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dc8ec4-92a6-4f8c-b75a-4cd4af5eb998",
   "metadata": {},
   "source": [
    "**Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbda4af-767e-4030-af0f-b411b106c592",
   "metadata": {},
   "source": [
    "From the Confusion matrix, we observe that the number of false positive and false negative has slightly decreased when using RandomSearch and hyperparamater tuning: \n",
    "- class 0: 20 -> 19\n",
    "- class 1: 23 -> 21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3accc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(rf_best, 'saved_models/random_forest_model.joblib')\n",
    "df_pred = pd.DataFrame({'Prediction': y_pred_rf})\n",
    "df_pred.to_csv(\"predictions/random_forest_model.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e25abd2-1dcc-4802-9431-668528a5bbc5",
   "metadata": {},
   "source": [
    "### 3) K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6909731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train_balanced, y_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a08701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already made predictions\n",
    "y_pred = knn.predict(X_test)\n",
    "y_pred_train = knn.predict(X_train_balanced)\n",
    "\n",
    "# accuracy\n",
    "train_accuracy = accuracy_score(y_train_balanced, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# precision\n",
    "train_precision = precision_score(y_train_balanced, y_pred_train)\n",
    "test_precision = precision_score(y_test, y_pred)\n",
    "\n",
    "# f1 score\n",
    "train_f1 = f1_score(y_train_balanced, y_pred_train)\n",
    "test_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'accuracy:')\n",
    "print(f'train: {train_accuracy}')\n",
    "print(f'test: {test_accuracy}')\n",
    "\n",
    "print(f'\\nprecision:')\n",
    "print(f'train: {train_precision}')\n",
    "print(f'test: {test_precision}')\n",
    "\n",
    "print(f'\\nf1:')\n",
    "print(f'train : {train_f1}')\n",
    "print(f'test: {test_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e831d55f",
   "metadata": {},
   "source": [
    "our models accuracy is good, but the f1-score for our abnormal data is not good enough, giving that we are working on anomaly detection, it is best to focus more on the f1-score than the accuracy for the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3168f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [1, 3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "grid_search = GridSearchCV(knn, param_grid, scoring=f1_scorer, cv=5, n_jobs=-1)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best F1-score found: \", grid_search.best_score_)\n",
    "\n",
    "best_knn = grid_search.best_estimator_\n",
    "y_pred = best_knn.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Final F1-Score: \", f1_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "disp_forest = ConfusionMatrixDisplay(conf_matrix, display_labels=best_knn.classes_)\n",
    "disp_forest.plot(cmap='Blues', values_format='d')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb232fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.loc[len(scores)] = ['KNN', accuracy, precision, f1]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_knn, 'saved_models/knn_model.joblib')\n",
    "df_pred = pd.DataFrame({'Prediction': y_pred})\n",
    "df_pred.to_csv(\"predictions/knn_model.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b5f93b-0061-4739-87e2-b951f67ea1a6",
   "metadata": {},
   "source": [
    "### 4) Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76aa16c-d874-4915-9098-97c437dbe763",
   "metadata": {},
   "source": [
    "**Gaussian Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa92699-93ee-4ad9-b1be-4b60a92e9ed8",
   "metadata": {},
   "source": [
    "As we have numerical features and distribution of each feature that seems quite like a sample of a Gaussian distribution. For such reasons, we will use $Gaussian Naive Bayes$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0801c3e-7e62-46ee-8c96-f749a5bb8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_balanced, y_train_balanced)\n",
    "gnb_preds = gnb.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "train_accuracy_gnb = accuracy_score(y_train_balanced, gnb.predict(X_train_balanced))\n",
    "test_accuracy_gnb = accuracy_score(y_test, gnb_preds)\n",
    "9967\n",
    "# precision\n",
    "train_precision_gnb = precision_score(y_train_balanced, gnb.predict(X_train_balanced))\n",
    "test_precision_gnb = precision_score(y_test, gnb_preds)\n",
    "\n",
    "# f1 score\n",
    "train_f1_gnb = f1_score(y_train_balanced, gnb.predict(X_train_balanced))\n",
    "test_f1_gnb = f1_score(y_test, gnb_preds)\n",
    "\n",
    "print(f'accuracy:')\n",
    "print(f'  train: {train_accuracy_gnb}')\n",
    "print(f'  test: {test_accuracy_gnb}')\n",
    "\n",
    "print(f'\\nprecision:')\n",
    "print(f'  train: {train_precision_gnb}')\n",
    "print(f'  test: {test_precision_gnb}')\n",
    "\n",
    "print(f'\\nf1:')\n",
    "print(f'  train: {train_f1_gnb}')\n",
    "print(f'  test: {test_f1_gnb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d20953-906d-416f-af4a-8d179db2152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "print(classification_report(y_test, gnb_preds))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_test, gnb_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=gnb.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfded7e-4fd8-4d4f-83d7-bfe9bf6ced48",
   "metadata": {},
   "source": [
    "Although we made sure to remove any dependant features, Naive Bayes still performing badly. This can be due to many reasons:  \n",
    "* So many outliers, which are among the main goal of this study. One outlier affects the mean and the variance. Thus, all the likelihood probabilities will be affected.  \n",
    "* The imbalanced data (ground truth) that we had to balance alongside the fact that $Naive Bayes$ makes a strong assumption of independence between the features.\n",
    "  \n",
    "To address this issue, we will hyperparameter tune the $GaussianNB$ using the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105d103b-94fa-4a84-b6cb-850b1f6fa011",
   "metadata": {},
   "source": [
    "**Hyperparameterized Gaussian Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d7a74d-6aff-4828-bfba-3e3d81e14774",
   "metadata": {},
   "source": [
    "The first parameter we trat is `var_smoothing`. It specifies the portion of the largest variance of all features to be added to variances for stability.  \n",
    "The next one is to make our features more or less normally distributed. Because, real life data is hardly normal. To do that, we use `PowerTransformer`.  \n",
    "Last but not least, we do cross validation using five splites repeated three time $Stratified K-Fold$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cc933a-2a14-4d66-864c-619814d840c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "cv_method = RepeatedStratifiedKFold(n_splits=5, \n",
    "                                    n_repeats=3, \n",
    "                                    random_state=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb757c-204e-48bf-a389-677b2cb39ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# parameters grid\n",
    "param_grid_nb = {\n",
    "    # smoothing variable represents 100 numbers from 0 to -9 spaced evenly on a log scale\n",
    "    'var_smoothing': np.logspace(0, -9, num=100),\n",
    "}\n",
    "\n",
    "# initializing the random search\n",
    "random_search_gnb = RandomizedSearchCV(\n",
    "    estimator=gnb,\n",
    "    param_distributions=param_grid_nb,\n",
    "    n_iter=100,\n",
    "    cv=cv_method,\n",
    "    random_state=42,\n",
    "    scoring='f1'\n",
    ")\n",
    "\n",
    "# fitting the random search\n",
    "X_test_transformed = PowerTransformer().fit_transform(X_test)\n",
    "random_search_gnb.fit(X_test_transformed, y_test)\n",
    "\n",
    "best_params_gnb = random_search_gnb.best_params_\n",
    "best_score_gnb = random_search_gnb.best_score_\n",
    "\n",
    "print(f'best Gaussian Naive Bayes parameters: {best_params_gnb}')\n",
    "print(f'best score (F1): {best_score_gnb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd5885-c9d6-43a4-9476-cce77b3524a8",
   "metadata": {},
   "source": [
    "Great results in train so far.  \n",
    "Now, we apply that actually on the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea75498-211a-4aed-a028-c96d94e4495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gnb = random_search_gnb.best_estimator_\n",
    "best_gnb.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "best_gnb_preds = best_gnb.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "train_accuracy_best_gnb = accuracy_score(y_train_balanced, best_gnb.predict(X_train_balanced))\n",
    "test_accuracy_best_gnb = accuracy_score(y_test, best_gnb_preds)\n",
    "\n",
    "# precision\n",
    "train_precision_best_gnb = precision_score(y_train_balanced, best_gnb.predict(X_train_balanced))\n",
    "test_precision_best_gnb = precision_score(y_test, best_gnb_preds)\n",
    "\n",
    "# f1 score\n",
    "train_f1_best_gnb = f1_score(y_train_balanced, best_gnb.predict(X_train_balanced))\n",
    "test_f1_best_gnb = f1_score(y_test, best_gnb_preds)\n",
    "\n",
    "print(f'accuracy:')\n",
    "print(f'  train: {train_accuracy_best_gnb}')\n",
    "print(f'  test: {test_accuracy_best_gnb}')\n",
    "\n",
    "print(f'\\nprecision:')\n",
    "print(f'  train: {train_precision_best_gnb}')\n",
    "print(f'  test: {test_precision_best_gnb}')\n",
    "\n",
    "print(f'\\nf1:')\n",
    "print(f'  train: {train_f1_best_gnb}')\n",
    "print(f'  test: {test_f1_best_gnb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34616a6-cf04-44d1-9608-981775ded871",
   "metadata": {},
   "source": [
    "Same with smoothing Gaussian Naive Bayes.  \n",
    "One method to handle the data imbalance is to use $ComplementNB$ which takes that into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643a84cb-151c-4791-9a94-3d73be0b8a9a",
   "metadata": {},
   "source": [
    "**Complement Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c51638-b51b-422f-a275-8f2b42b2b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "cnb = ComplementNB(fit_prior=False, norm=False)\n",
    "cnb.fit(X_train_scaled, y_train_balanced)\n",
    "cnb_preds = cnb.predict(X_test_scaled)\n",
    "\n",
    "# accuracy\n",
    "train_accuracy_cnb = accuracy_score(y_train_balanced, cnb.predict(X_train_scaled))\n",
    "test_accuracy_cnb = accuracy_score(y_test, cnb_preds)\n",
    "\n",
    "# precision\n",
    "train_precision_cnb = precision_score(y_train_balanced, cnb.predict(X_train_scaled))\n",
    "test_precision_cnb = precision_score(y_test, cnb_preds)\n",
    "\n",
    "# f1 score\n",
    "train_f1_cnb = f1_score(y_train_balanced, cnb.predict(X_train_scaled))\n",
    "test_f1_cnb = f1_score(y_test, cnb_preds)\n",
    "\n",
    "print(f'accuracy:')\n",
    "print(f'  train: {train_accuracy_cnb}')\n",
    "print(f'  test: {test_accuracy_cnb}')\n",
    "\n",
    "print(f'\\nprecision:')\n",
    "print(f'  train: {train_precision_cnb}')\n",
    "print(f'  test: {test_precision_cnb}')\n",
    "\n",
    "print(f'\\nf1:')\n",
    "print(f'  train: {train_f1_cnb}')\n",
    "print(f'  test: {test_f1_cnb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a6ada-6830-40d7-b769-1493071846a9",
   "metadata": {},
   "source": [
    "This one went worse than the previous techniques of Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5d97a-6c07-4db7-a1d8-73601cea5137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to scores dataframe\n",
    "scores.loc[len(scores)] = ['Gaussian Naive Bayes', test_accuracy_gnb, test_precision_gnb, test_f1_gnb]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc4733-d7d0-4063-84a4-ac674af6041a",
   "metadata": {},
   "source": [
    "As noticeable, Gaussian Naive Bayes did perform quite well. Yet, worse than previous classifiers. This is due to the nature of the features, half of them follow a similar to Normal distribution and the rest follow a Bernoulli distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ed0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(gnb, 'saved_models/naive_bayes_model.joblib')\n",
    "df_pred = pd.DataFrame({'Prediction': best_gnb_preds})\n",
    "df_pred.to_csv(\"predictions/naive_bayes_model.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69997a4e-f10a-4a03-87f3-1d3149bab329",
   "metadata": {},
   "source": [
    "### 5) Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ed099-8ef3-4a15-a0b4-7a36be0a2fd4",
   "metadata": {},
   "source": [
    "**Support Vector Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd0b95c-73b4-4f67-a9be-abcdf50daa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='rbf', degree=3)\n",
    "svm.fit(X_train_balanced, y_train_balanced)\n",
    "svm_preds = svm.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "train_accuracy_svm = accuracy_score(y_train_balanced, svm.predict(X_train_balanced))\n",
    "test_accuracy_svm = accuracy_score(y_test, svm_preds)\n",
    "\n",
    "# precision\n",
    "train_precision_svm = precision_score(y_train_balanced, svm.predict(X_train_balanced))\n",
    "test_precision_svm = precision_score(y_test, svm_preds)\n",
    "\n",
    "# f1 score\n",
    "train_f1_svm = f1_score(y_train_balanced, svm.predict(X_train_balanced))\n",
    "test_f1_svm = f1_score(y_test, svm_preds)\n",
    "\n",
    "print(f'accuracy:')\n",
    "print(f'  train: {train_accuracy_svm}')\n",
    "print(f'  test: {test_accuracy_svm}')\n",
    "\n",
    "print(f'\\nprecision:')\n",
    "print(f'  train: {train_precision_svm}')\n",
    "print(f'  test: {test_precision_svm}')\n",
    "\n",
    "print(f'\\nf1:')\n",
    "print(f'  train: {train_f1_svm}')\n",
    "print(f'  test: {test_f1_svm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2059da-aacb-4cc4-b7d0-aff90e215a8d",
   "metadata": {},
   "source": [
    "**Hyperparameterized SVC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63577da-2ea6-4792-92db-574721a6048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters grid\n",
    "param_grid_svc = {\n",
    "    'kernel': ['rbf', 'linear', 'poly'],\n",
    "    'degree': [1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "# initializing the random search\n",
    "random_search_svc = RandomizedSearchCV(\n",
    "    estimator=svm,\n",
    "    param_distributions=param_grid_svc,\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    "    scoring='f1'\n",
    ")\n",
    "\n",
    "# fitting the random search\n",
    "random_search_svc.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "best_params_svc = random_search_svc.best_params_\n",
    "best_score_svc = random_search_svc.best_score_\n",
    "\n",
    "print(f'best SVC parameters: {best_params_svc}')\n",
    "print(f'best score (F1): {best_score_svc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902230b1-df82-4622-9f49-72a0674e79e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svc = random_search_svc.best_estimator_\n",
    "best_svc.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "best_svc_preds = best_svc.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "train_accuracy_best_svc = accuracy_score(y_train_balanced, best_svc.predict(X_train_balanced))\n",
    "test_accuracy_best_svc = accuracy_score(y_test, best_svc_preds)\n",
    "\n",
    "# precision\n",
    "train_precision_best_svc = precision_score(y_train_balanced, best_svc.predict(X_train_balanced))\n",
    "test_precision_best_svc = precision_score(y_test, best_svc_preds)\n",
    "\n",
    "# f1 score\n",
    "train_f1_best_svc = f1_score(y_train_balanced, best_svc.predict(X_train_balanced))\n",
    "test_f1_best_svc = f1_score(y_test, best_svc_preds)\n",
    "\n",
    "print(f'accuracy:')\n",
    "print(f'  train: {train_accuracy_best_svc}')\n",
    "print(f'  test: {test_accuracy_best_svc}')\n",
    "\n",
    "print(f'\\nprecision:')\n",
    "print(f'  train: {train_precision_best_svc}')\n",
    "print(f'  test: {test_precision_best_svc}')\n",
    "\n",
    "print(f'\\nf1:')\n",
    "print(f'  train: {train_f1_best_svc}')\n",
    "print(f'  test: {test_f1_best_svc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea90ee7-0048-4260-a1e8-c3ada1123623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "print(classification_report(y_test, best_svc_preds))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_test, best_svc_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_svc.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b64abf9-0824-4637-baff-8f7334d1933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to scores dataframe\n",
    "scores.loc[len(scores)] = ['SVM', test_accuracy_svm, test_precision_svm, test_f1_svm]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5779c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(svm, 'saved_models/svm_model.joblib')\n",
    "df_pred = pd.DataFrame({'Prediction': best_svc_preds})\n",
    "df_pred.to_csv(\"predictions/svm_model.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2491cf",
   "metadata": {},
   "source": [
    "### 6) Artificial Neural Networks (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46464c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, input_shape=(10, ), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca2f85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Define precision metric\n",
    "def precision_m(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "# Define recall metric\n",
    "def recall_m(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "# Define F1 score metric\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1238864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", \n",
    "              loss=\"binary_crossentropy\", \n",
    "              metrics=['accuracy', precision_m, recall_m, f1_m])\n",
    "model.fit(X_train_balanced, y_train_balanced, epochs=100)\n",
    "\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f44424",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = np.round(model.predict(X_test))\n",
    "accuracy = accuracy_score(preds_test, y_test)\n",
    "print('the accuracy score for the neural network model in testing is: ', accuracy)\n",
    "\n",
    "preds_train = np.round(model.predict(X_train_balanced))\n",
    "accuracy = accuracy_score(preds_train, y_train_balanced)\n",
    "print('the accuracy score for the neural network model in training is: ', accuracy)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Final F1-Score: \", f1_score(y_test, preds_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, preds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6214be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "def create_model(optimizer='adam', init='glorot_uniform', activation='relu', neurons=256):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_shape=(X_train_balanced.shape[1],), kernel_initializer=init, activation=activation))\n",
    "    model.add(Dense(neurons // 2, kernel_initializer=init, activation=activation))\n",
    "    model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6516be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'optimizer': ['adam', 'sgd'],\n",
    "    'init': ['glorot_uniform', 'normal', 'uniform'],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'neurons': [64, 128, 256],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [50, 100, 150]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46f7b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(model=create_model, verbose=0, neurons=256, init='glorot_uniform', activation='relu')\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, scoring='f1', cv=3, verbose=1, n_jobs=-1)\n",
    "\n",
    "random_search_result = random_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"Best parameters found: \", random_search_result.best_params_)\n",
    "print(\"Best F1-score found: \", random_search_result.best_score_)\n",
    "\n",
    "best_model = random_search_result.best_estimator_\n",
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94059fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced2b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = np.round(best_model.predict(X_train_balanced))\n",
    "\n",
    "print('Evaluation of the model on training data:')\n",
    "accuracy = accuracy_score(y_train_balanced, preds_train)\n",
    "precision = precision_score(y_train_balanced, preds_train)\n",
    "recall = recall_score(y_train_balanced, preds_train)\n",
    "f1 = f1_score(y_train_balanced, preds_train)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5718ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.model_.save('saved_models/ann_model.keras')\n",
    "df_pred = pd.DataFrame({'Prediction': y_pred})\n",
    "df_pred.to_csv(\"predictions/ann_model.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf2585f",
   "metadata": {},
   "source": [
    "# 4. Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9017be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_models_dir = 'saved_models'\n",
    "saved_predictions_dir = 'predictions'\n",
    "\n",
    "models = {}\n",
    "\n",
    "for filename in os.listdir(saved_models_dir):\n",
    "\n",
    "    filepath = os.path.join(saved_models_dir, filename)\n",
    "    \n",
    "    if filename.endswith('.joblib'):\n",
    "        model = \"\"\n",
    "    elif filename.endswith('.keras'):\n",
    "        model = \"\"\n",
    "    else:\n",
    "        # Skip if the file is not a supported model type\n",
    "        continue\n",
    "    \n",
    "    model_name = os.path.splitext(filename)[0]\n",
    "    \n",
    "    models[model_name] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa25c330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_test, model_name):\n",
    "    \n",
    "    saved_predictions_path = saved_predictions_dir + '/' + model_name + '.csv'\n",
    "\n",
    "    if os.path.exists(saved_predictions_path):\n",
    "        preds = pd.read_csv(saved_predictions_path)\n",
    "        preds = preds['Prediction']\n",
    "    else:\n",
    "        print(\"prediction doesn't exits\")\n",
    "    \n",
    "    \n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    precision = precision_score(y_test, preds)\n",
    "    recall = recall_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, cm, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedee686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    accuracy, precision, recall, f1, cm, preds = evaluate_model(model, X_test, y_test, name)\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': cm,\n",
    "        'predictions': preds\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b428f037",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 16))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "best_models = {}\n",
    "worst_models = {}\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    sns.barplot(x='Model', y=metric, data=results_df, palette='viridis', ax=axes[i])\n",
    "    axes[i].set_title(f'Model Comparison - {metric}')\n",
    "    axes[i].set_xlabel('Model')\n",
    "    axes[i].set_ylabel(metric)\n",
    "    \n",
    "    # Find best and worst model for each metric\n",
    "    best_model = results_df.loc[results_df[metric].idxmax()]['Model']\n",
    "    worst_model = results_df.loc[results_df[metric].idxmin()]['Model']\n",
    "    best_models[metric] = best_model\n",
    "    worst_models[metric] = worst_model\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print best and worst models for each metric\n",
    "for metric, best_model in best_models.items():\n",
    "    print(f'Best model for {metric}: {best_model}')\n",
    "    \n",
    "for metric, worst_model in worst_models.items():\n",
    "    print(f'Worst model for {metric}: {worst_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65901807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "fpr_list = []\n",
    "fnr_list = []\n",
    "models = []\n",
    "\n",
    "for result in results:\n",
    "    cm = result['Confusion Matrix']\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    \n",
    "    FPR = FP / (FP + TN)\n",
    "    FNR = FN / (FN + TP)\n",
    "    \n",
    "    fpr_list.append(FPR)\n",
    "    fnr_list.append(FNR)\n",
    "    models.append(result['Model'])\n",
    "\n",
    "# Create bar charts for FPR and FNR\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Bar chart for FPR\n",
    "sns.barplot(x=models, y=fpr_list, ax=ax[0], palette='Blues_d')\n",
    "ax[0].set_title('False Positive Rate (FPR)')\n",
    "ax[0].set_ylabel('Rate')\n",
    "ax[0].set_xlabel('Model')\n",
    "\n",
    "# Bar chart for FNR\n",
    "sns.barplot(x=models, y=fnr_list, ax=ax[1], palette='Blues_d')\n",
    "ax[1].set_title('False Negative Rate (FNR)')\n",
    "ax[1].set_ylabel('Rate')\n",
    "ax[1].set_xlabel('Model')\n",
    "\n",
    "for ax in ax:\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cc80c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define profit and loss values\n",
    "profit_tp = 100\n",
    "profit_tn = 0\n",
    "loss_fp = 10\n",
    "loss_fn = 700\n",
    "\n",
    "profits_losses = []\n",
    "\n",
    "for index, row in results_df.iterrows():\n",
    "    \n",
    "    tn, fp, fn, tp = row['Confusion Matrix'].ravel()\n",
    "    \n",
    "    profit = tp * profit_tp + tn * profit_tn\n",
    "    loss = fp * loss_fp + fn * loss_fn\n",
    "    net_profit_loss = profit - loss\n",
    "    \n",
    "    profits_losses.append(net_profit_loss)\n",
    "\n",
    "results_df['Profit_Loss'] = profits_losses\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Model', y='Profit_Loss', data=results_df, palette='viridis')\n",
    "plt.title('Model Comparison - Profit and Loss')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Profit and Loss')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Print the best and worst models for profit and loss\n",
    "best_profit_model = results_df.loc[results_df['Profit_Loss'].idxmax()]\n",
    "worst_profit_model = results_df.loc[results_df['Profit_Loss'].idxmin()]\n",
    "print(f\"Best Profit Model: {best_profit_model['Model']} with Profit and Loss: {best_profit_model['Profit_Loss']}\")\n",
    "print(f\"Worst Profit Model: {worst_profit_model['Model']} with Profit and Loss: {worst_profit_model['Profit_Loss']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "muo env",
   "language": "python",
   "name": "muo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
