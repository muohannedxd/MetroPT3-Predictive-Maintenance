{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f3aeb1-7ef0-488a-a91f-57c9f1461e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ad14a9-cc80-40fa-be60-c46718803923",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/MetroPT3(AirCompressor).csv', index_col = False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db36fe69",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning and Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242d02e6",
   "metadata": {},
   "source": [
    "According to the documentation, the following preprocessing steps have been conducted before publishing the data:\n",
    "\n",
    "- Data segmentation\n",
    "- Normalization\n",
    "- Feature Extraction\n",
    "\n",
    "Thus, we do not need to apply them in our work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39eac0f",
   "metadata": {},
   "source": [
    "### 1) Overview "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a0f6d3-6506-4d21-ad5e-038457ada2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of null values: {data.isna().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a10084d-a437-48db-9147-3d799038f9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of duplicates: {data.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986aa2c-f6c6-4ba0-8af1-d541e0ccb6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfb9f93",
   "metadata": {},
   "source": [
    "### 2) drop unnecessary columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ff329-4364-40ea-a3a7-1ac7665a5106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unecessary columns\n",
    "data.drop(['Unnamed: 0'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849b9d39",
   "metadata": {},
   "source": [
    "### 3) Add a label Column \n",
    "From the failure information table provided int the data description file below, we will try to label the data and evaluate the effectiveness of failure prediction algorithms: \n",
    "\n",
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902e9b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = data.copy()\n",
    "labeled_data['status'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6092ea",
   "metadata": {},
   "source": [
    "#### Converting the timestamp column into pandas.DateTime data type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95922fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the timestamp to datetime\n",
    "labeled_data['timestamp'] = pd.to_datetime(labeled_data['timestamp'], format = '%Y-%m-%d %H:%M:%S')\n",
    "print(\"current data type of timestamp: \", labeled_data['timestamp'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14cb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to convert time to pandas.dateTime \n",
    "def convert_time(X):\n",
    "    result =[]\n",
    "    for x in X:\n",
    "        result.append(pd.to_datetime(x, format = '%Y-%m-%d %H:%M:%S'))\n",
    "    return result\n",
    "\n",
    "failure_start_time = convert_time([\"2020-04-18 00:00:00\", \"2020-05-29 23:30:00\", \"2020-06-05 10:00:00\", \"2020-07-15 14:30:00\"])\n",
    "failure_end_time = convert_time([\"2020-04-18 23:59:00\", \"2020-05-30 06:00:00\", \"2020-06-07 14:30:00\", \"2020-07-15 19:00:00\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e39fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through the data and label the data\n",
    "for start, end in zip(failure_start_time, failure_end_time):\n",
    "    labeled_data.loc[(labeled_data['timestamp'] >= start) & (labeled_data['timestamp'] <= end), 'status'] = 1\n",
    "    #check if any failures were missed or\n",
    "    print(f\"number of failures between {start} and {end}: {labeled_data.loc[(labeled_data['timestamp'] >= start) & (labeled_data['timestamp'] <= end), 'status'].sum()}\")\n",
    "    \n",
    "print(f\"number of failures: {labeled_data['status'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af245553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for positive class imbalance\n",
    "print(f\"Example of Failure state \\n {labeled_data[labeled_data['status']==1].head()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd1448",
   "metadata": {},
   "source": [
    "### 4) Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1a1e02-6c19-49d6-9600-025c6ad3c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data.status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0e944e-24a1-4073-828f-07c463a0fe34",
   "metadata": {},
   "source": [
    "The number of negative values (normal cases) is way too large compared to the positive class (around 30k positive samples and 1500k negative samples). Then, we are running into an Imbalaned Dataset. It is expected since we are dealing with a predictive maintenance problem.  \n",
    "To address this issue, we ought to balance our data. There are various techniques to balance it. One of them is **Undersampling**. For that, we will be using the `RandomUnderSampler` from `imblearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5156dd3-ff17-4366-a5f9-66144f81bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "y = labeled_data.status\n",
    "X = labeled_data.iloc[:, :-1]\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "balanced_data = X_resampled\n",
    "balanced_data['status'] = y_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75059c6b-7e85-47a0-9845-ae3be23250ee",
   "metadata": {},
   "source": [
    "`balanced_data` is supposed to be balanced now. Let us check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b787c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value counts from the imbalanced dataset\n",
    "imbalanced_class_counts = labeled_data['status'].value_counts()\n",
    "\n",
    "# value counts from the balanced dataset\n",
    "balanced_class_counts = balanced_data['status'].value_counts()\n",
    "\n",
    "# plot pie charts to show the class distribution difference\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].pie(\n",
    "    imbalanced_class_counts,\n",
    "    labels = ['Negative', 'Positive'],\n",
    "    autopct = '%1.1f%%',\n",
    "    startangle = 90,\n",
    "    colors = ['lightpink', 'lightblue']\n",
    ")\n",
    "axes[0].set_title('Before Undersampling')\n",
    "\n",
    "axes[1].pie(\n",
    "    balanced_class_counts,\n",
    "    labels = ['Negative', 'Positive'],\n",
    "    autopct = '%1.1f%%',\n",
    "    startangle = 90,\n",
    "    colors = ['lightpink', 'lightblue']\n",
    ")\n",
    "axes[1].set_title('After Undersampling')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3777688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2d8e03",
   "metadata": {},
   "source": [
    "### 5) Checking for outliers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb42acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outliers(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    num_outliers = len(outliers)\n",
    "    print(f\"Number of outliers in {column}: {num_outliers}\")\n",
    "    return outliers\n",
    "\n",
    "def remove_outliers(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers_removed = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n",
    "    num_removed = len(data) - len(outliers_removed)\n",
    "    print(f\"Number of outliers removed from {column}: {num_removed}\\n\")\n",
    "    return outliers_removed\n",
    "\n",
    "# First, identify outliers\n",
    "clean_data = balanced_data.copy()\n",
    "for col in clean_data:\n",
    "    if col not in ['timestamp', 'status']:\n",
    "        outliers = identify_outliers(clean_data, col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e786b310",
   "metadata": {},
   "source": [
    "the features: ['COMP', 'DV_eletric','Towers', 'MPG','LPS','Pressure_switch','Oil_level','Caudal_impulses'] are binary features. So we do not remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4587e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in clean_data:\n",
    "    if col not in ['timestamp', 'status', 'LPS', 'Pressure_switch', 'Oil_level', 'Caudal_impulses']:\n",
    "        cleaned_data = remove_outliers(clean_data, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356346ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate the columns with the binary values\n",
    "binary_cols = ['LPS', 'Pressure_switch', 'Oil_level', 'Caudal_impulses']\n",
    "#Ensure the the binary data is binary\n",
    "cleaned_data[binary_cols] = cleaned_data[binary_cols].apply(np.round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of unique values in each column\n",
    "for col in cleaned_data.columns:\n",
    "    print(f\"number of unique values in {col}: {cleaned_data[col].nunique()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd56659",
   "metadata": {},
   "source": [
    "# 2. Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3b218a",
   "metadata": {},
   "source": [
    "### 1) Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation \n",
    "correlation = cleaned_data.corr()\n",
    "plt.figure(figsize = (10, 10))\n",
    "sns.heatmap(correlation, annot = True, cmap = 'coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe5a44",
   "metadata": {},
   "source": [
    "From the above correlation heatmap,  we can see that our target feature **\"status\"** has a strong correlation with these features: TP2, H1, DV_pressure, Oil_temparature, Motor_current, COMP, DV_electric and MPG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b845db8",
   "metadata": {},
   "source": [
    "### 2) Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5724c60",
   "metadata": {},
   "source": [
    "1. Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44513f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize all the features outliers in one plot \n",
    "sns.set(rc={'figure.figsize':(20,8.27)})\n",
    "sns.boxplot(data = cleaned_data.drop(['timestamp', 'status'], axis = 1))\n",
    "# plt.xticks(rotation = 45)\n",
    "plt.title('Boxplot of all features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee1ba6",
   "metadata": {},
   "source": [
    "2. Probability distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0acb70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module='seaborn')\n",
    "\n",
    "#visualize the probability distribution of all the features\n",
    "def plot_col_distribution(data):\n",
    "    fig, axes = plt.subplots(4, 4, figsize = (20, 10))\n",
    "    axes = axes.flatten()\n",
    "    for i, col in enumerate(data.columns):\n",
    "        data[col] = data[col].replace([np.inf, -np.inf], np.nan)\n",
    "        sns.histplot(data[col], ax = axes[i], kde=True)\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_col_distribution(cleaned_data.drop(['timestamp', 'status'], axis = 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b83150",
   "metadata": {},
   "source": [
    "3. Time series plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35580505",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.iloc[:,:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec9963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorganize according to timestamp \n",
    "cleaned_data.sort_values('timestamp', inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb99ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series\n",
    "cleaned_data.iloc[:,:16].plot(\n",
    "        subplots =True,\n",
    "        layout=(6, 3),\n",
    "        figsize=(22,22),\n",
    "        fontsize=10, \n",
    "        linewidth=1,\n",
    "        sharex = False, \n",
    "        title='Visualization of the Original Time Series')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767d78cf-7d8a-44d9-9038-8c9e0202ffcf",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180c51d4-bc9e-4ed5-b2b0-8a2f11222232",
   "metadata": {},
   "source": [
    "### 1) Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2b3898-e9ba-4469-8b53-ecf05cf023ca",
   "metadata": {},
   "source": [
    "Before Modeling and classifying our data to be able to predict when we need maintenance, we need to split our data into train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f124f6-54f6-4a34-96c1-ca84c8571c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = cleaned_data.status\n",
    "X = cleaned_data.iloc[:, :-1].drop(columns=['timestamp'])\n",
    "\n",
    "X_train, X_test,y_train, y_test = train_test_split(X, y,\n",
    "                                   random_state=42, \n",
    "                                   test_size=0.25, \n",
    "                                   shuffle=True)\n",
    "\n",
    "print(f'shape of X_train: {X_train.shape}')\n",
    "print(f'shape of X_test: {X_test.shape}')\n",
    "print(f'shape of y_train: {y_train.shape}')\n",
    "print(f'shape of y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb43c5e-878e-46c0-8fa9-8ef426b25ae5",
   "metadata": {},
   "source": [
    "### 2) Evaluation Protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e10ff2-e055-4e90-b86d-e236d937a73e",
   "metadata": {},
   "source": [
    "Reminder that the aim of this study is to predict failures and the need of maintenance in an\n",
    "urban metro public transportation service. To assess the fit and how good each model performed, we will evaluate it using the following metrics:\n",
    "* **Accuracy:** measures the overall correctness of the model since our data is balanced:\n",
    "\\begin{align*}\n",
    "    Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "\\end{align*}\n",
    "\n",
    "* **Precision:** the goal is to maximize the positive (failure) predictions when they were originally failures (TP) and minimize negative calls (FN) which are failure values predicted as non-failure by the model:\n",
    "\\begin{align*}\n",
    "    Precision = \\frac{TP}{TP + FN}\n",
    "\\end{align*}\n",
    "\n",
    "* **F1 Score**: harmonic mean of Precision and Recall:\n",
    "\\begin{align*}\n",
    "    F1 Score = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d8320c-4455-4dfb-94c5-30321017b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4d7cb0-4cfa-422b-a69b-83dd3c526abd",
   "metadata": {},
   "source": [
    "We maintain a dataframe `scores` to hold the scores of each model we will study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bed47f-5bec-4ed0-9427-db5dee9d8c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(columns=['model', 'accuracy', 'precision', 'f1'])\n",
    "scores # should be empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56743e17-6e81-4874-beac-64df61126971",
   "metadata": {},
   "source": [
    "### 3) Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e815f9-d4d6-439d-8fde-9c664decfab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "dt_preds = dt.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "train_accuracy_dt = accuracy_score(dt.predict(X_train), y_train)\n",
    "test_accuracy_dt = accuracy_score(dt_preds, y_test)\n",
    "\n",
    "# precision\n",
    "train_precision_dt = precision_score(dt.predict(X_train), y_train, average='macro')\n",
    "test_precision_dt = precision_score(dt_preds, y_test, average='macro')\n",
    "\n",
    "# f1 score\n",
    "train_f1_dt = f1_score(dt.predict(X_train), y_train, average='macro')\n",
    "test_f1_dt = f1_score(dt_preds, y_test, average='macro')\n",
    "\n",
    "print(f'accuracy:')\n",
    "print(f'  train: {train_accuracy_dt}')\n",
    "print(f'  test: {test_accuracy_dt}')\n",
    "\n",
    "print(f'\\nprecision:')\n",
    "print(f'  train: {train_precision_dt}')\n",
    "print(f'  test: {test_precision_dt}')\n",
    "\n",
    "print(f'\\nf1:')\n",
    "print(f'  train: {train_f1_dt}')\n",
    "print(f'  test: {test_f1_dt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a5d6c7-8ec1-4516-8c3f-29f39045005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to scores dataframe\n",
    "scores.loc[len(scores)] = ['Decision Tree', test_accuracy_dt, test_precision_dt, test_f1_dt]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0abdc-b426-4526-ab6a-4476968f4ab7",
   "metadata": {},
   "source": [
    "### 4) Hyperparameterized Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03596e4-4854-4b98-b4cc-7123c48a7995",
   "metadata": {},
   "source": [
    "Here, we are going to find the best Decision Tree Classifier. That is, we aim to find its parameters that best maximize the score. This is done by **Random Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7e09ff-7fe9-4a00-aef6-bc8ba4c63b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "dt_params = {\n",
    "    'criterion': ['gini'],\n",
    "    'max_depth': randint(1, 50),\n",
    "    'min_samples_split': randint(2, 11),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_dt = RandomizedSearchCV(\n",
    "    estimator=dt,\n",
    "    param_distributions=dt_params,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    n_jobs=1,\n",
    "    random_state=42,\n",
    "    scoring='f1_macro'\n",
    ")\n",
    "\n",
    "random_search_dt.fit(X_train, y_train)\n",
    "\n",
    "best_params_dt = random_search_dt.best_params_\n",
    "best_score_dt = random_search_dt.best_score_\n",
    "\n",
    "print(f'best decision tree parameters: {best_params_dt}')\n",
    "print(f'best score (F1): {best_score_dt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e87540-abb9-4093-b977-257d943cef0f",
   "metadata": {},
   "source": [
    "Now, we model with teh best **Decision Tree** best on the **Random Search**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e64d0ca-25de-4a95-ae8d-9733d2d73e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dt = random_search_dt.best_estimator_\n",
    "best_dt.fit(X_train, y_train)\n",
    "best_dt_preds = best_dt.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "train_accuracy_best_dt = accuracy_score(best_dt.predict(X_train), y_train)\n",
    "test_accuracy_best_dt = accuracy_score(best_dt_preds, y_test)\n",
    "\n",
    "# precision\n",
    "train_precision_best_dt = precision_score(best_dt.predict(X_train), y_train, average='macro')\n",
    "test_precision_best_dt = precision_score(best_dt_preds, y_test, average='macro')\n",
    "\n",
    "# f1 score\n",
    "train_f1_best_dt = f1_score(best_dt.predict(X_train), y_train, average='macro')\n",
    "test_f1_best_dt = f1_score(best_dt_preds, y_test, average='macro')\n",
    "\n",
    "print(f'accuracy:')\n",
    "print(f'  train: {train_accuracy_dt}')\n",
    "print(f'  test: {test_accuracy_dt}')\n",
    "\n",
    "print(f'\\nprecision:')\n",
    "print(f'  train: {train_precision_dt}')\n",
    "print(f'  test: {test_precision_dt}')\n",
    "\n",
    "print(f'\\nf1:')\n",
    "print(f'  train: {train_f1_dt}')\n",
    "print(f'  test: {test_f1_dt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1db338-6102-4ab0-9285-d994f8fbc5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to scores dataframe\n",
    "scores.loc[len(scores)] = ['Hyperparametirized Decision Tree', test_accuracy_best_dt, test_precision_best_dt, test_f1_best_dt]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621df7f7-ec02-4baf-bb8d-41f214884dd1",
   "metadata": {},
   "source": [
    "### 5) Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e25abd2-1dcc-4802-9431-668528a5bbc5",
   "metadata": {},
   "source": [
    "### 6) K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b5f93b-0061-4739-87e2-b951f67ea1a6",
   "metadata": {},
   "source": [
    "### 7) Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0801c3e-7e62-46ee-8c96-f749a5bb8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "gnb_preds = gnb.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "train_accuracy_gnb = accuracy_score(gnb.predict(X_train), y_train)\n",
    "test_accuracy_gnb = accuracy_score(gnb_preds, y_test)\n",
    "\n",
    "# precision\n",
    "train_precision_gnb = precision_score(gnb.predict(X_train), y_train, average='macro')\n",
    "test_precision_gnb = precision_score(gnb_preds, y_test, average='macro')\n",
    "\n",
    "# f1 score\n",
    "train_f1_gnb = f1_score(gnb.predict(X_train), y_train, average='macro')\n",
    "test_f1_gnb = f1_score(gnb_preds, y_test, average='macro')\n",
    "\n",
    "print(f'accuracy:')\n",
    "print(f'  train: {train_accuracy_gnb}')\n",
    "print(f'  test: {test_accuracy_gnb}')\n",
    "\n",
    "print(f'\\nprecision:')\n",
    "print(f'  train: {train_precision_gnb}')\n",
    "print(f'  test: {test_precision_gnb}')\n",
    "\n",
    "print(f'\\nf1:')\n",
    "print(f'  train: {train_f1_gnb}')\n",
    "print(f'  test: {test_f1_gnb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5d97a-6c07-4db7-a1d8-73601cea5137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to scores dataframe\n",
    "scores.loc[len(scores)] = ['Gaussian Naive Bayes', test_accuracy_gnb, test_precision_gnb, test_f1_gnb]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc4733-d7d0-4063-84a4-ac674af6041a",
   "metadata": {},
   "source": [
    "As noticeable, Gaussian Naive Bayes did perform quite well. Yet, worse than previous classifiers. This is due to the nature of the features, half of them follow a similar to Normal distribution and the rest follow a Bernoulli distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69997a4e-f10a-4a03-87f3-1d3149bab329",
   "metadata": {},
   "source": [
    "### 8) Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd0b95c-73b4-4f67-a9be-abcdf50daa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='linear', degree=3)\n",
    "svm.fit(X_train, y_train)\n",
    "svm_preds = svm.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "train_accuracy_svm = accuracy_score(svm.predict(X_train), y_train)\n",
    "test_accuracy_svm = accuracy_score(svm_preds, y_test)\n",
    "\n",
    "# precision\n",
    "train_precision_svm = precision_score(svm.predict(X_train), y_train, average='macro')\n",
    "test_precision_svm = precision_score(svm_preds, y_test, average='macro')\n",
    "\n",
    "# f1 score\n",
    "train_f1_svm = f1_score(svm.predict(X_train), y_train, average='macro')\n",
    "test_f1_svm = f1_score(svm_preds, y_test, average='macro')\n",
    "\n",
    "print(f'accuracy:')\n",
    "print(f'  train: {train_accuracy_svm}')\n",
    "print(f'  test: {test_accuracy_svm}')\n",
    "\n",
    "print(f'\\nprecision:')\n",
    "print(f'  train: {train_precision_svm}')\n",
    "print(f'  test: {test_precision_svm}')\n",
    "\n",
    "print(f'\\nf1:')\n",
    "print(f'  train: {train_f1_svm}')\n",
    "print(f'  test: {test_f1_svm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b64abf9-0824-4637-baff-8f7334d1933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to scores dataframe\n",
    "scores.loc[len(scores)] = ['SVM', test_accuracy_svm, test_precision_svm, test_f1_svm]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19021f54-daa7-4e52-90d3-f6aec5f416e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "muo env",
   "language": "python",
   "name": "muo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
