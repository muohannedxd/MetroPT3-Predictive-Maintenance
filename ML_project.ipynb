{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f3aeb1-7ef0-488a-a91f-57c9f1461e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ad14a9-cc80-40fa-be60-c46718803923",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/MetroPT3(AirCompressor).csv', index_col = False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db36fe69",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning and Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242d02e6",
   "metadata": {},
   "source": [
    "According to the documentation, the following preprocessing steps have been conducted before publishing the data:\n",
    "\n",
    "- Data segmentation\n",
    "- Normalization\n",
    "- Feature Extraction\n",
    "\n",
    "Thus, we do not need to apply them in our work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39eac0f",
   "metadata": {},
   "source": [
    "### 1) Overview "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a0f6d3-6506-4d21-ad5e-038457ada2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of null values: {data.isna().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a10084d-a437-48db-9147-3d799038f9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of duplicates: {data.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986aa2c-f6c6-4ba0-8af1-d541e0ccb6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'shape: {data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfb9f93",
   "metadata": {},
   "source": [
    "### 2) drop unnecessary columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ff329-4364-40ea-a3a7-1ac7665a5106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unecessary columns\n",
    "data.drop(['Unnamed: 0'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849b9d39",
   "metadata": {},
   "source": [
    "### 3) Add a label Column \n",
    "From the failure information table provided int the data description file below, we will try to label the data and evaluate the effectiveness of failure prediction algorithms: \n",
    "\n",
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902e9b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = data.copy()\n",
    "labeled_data['status'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6092ea",
   "metadata": {},
   "source": [
    "#### Converting the timestamp column into pandas.DateTime data type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95922fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the timestamp to datetime\n",
    "labeled_data['timestamp'] = pd.to_datetime(labeled_data['timestamp'], format = '%Y-%m-%d %H:%M:%S')\n",
    "print(\"current data type of timestamp: \", labeled_data['timestamp'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14cb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to convert time to pandas.dateTime \n",
    "def convert_time(X):\n",
    "    result =[]\n",
    "    for x in X:\n",
    "        result.append(pd.to_datetime(x, format = '%Y-%m-%d %H:%M:%S'))\n",
    "    return result\n",
    "\n",
    "failure_start_time = convert_time([\"2020-04-18 00:00:00\", \"2020-05-29 23:30:00\", \"2020-06-05 10:00:00\", \"2020-07-15 14:30:00\"])\n",
    "failure_end_time = convert_time([\"2020-04-18 23:59:00\", \"2020-05-30 06:00:00\", \"2020-06-07 14:30:00\", \"2020-07-15 19:00:00\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e39fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through the data and label the data\n",
    "for start, end in zip(failure_start_time, failure_end_time):\n",
    "    labeled_data.loc[(labeled_data['timestamp'] >= start) & (labeled_data['timestamp'] <= end), 'status'] = 1\n",
    "    #check if any failures were missed or\n",
    "    print(f\"number of failures between {start} and {end}: {labeled_data.loc[(labeled_data['timestamp'] >= start) & (labeled_data['timestamp'] <= end), 'status'].sum()}\")\n",
    "    \n",
    "print(f\"number of failures: {labeled_data['status'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af245553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for positive class imbalance\n",
    "print(f\"Example of Failure state \\n {labeled_data[labeled_data['status']==1].head()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e2931a-00ae-479d-a2c5-1cc6ee1ea3dc",
   "metadata": {},
   "source": [
    "### 4) Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aeed59-07d3-4bae-8178-f574acaa7564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = labeled_data.status\n",
    "X = labeled_data.drop(columns=['status'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                   random_state=42, \n",
    "                                   test_size=0.25, \n",
    "                                   shuffle=True)\n",
    "\n",
    "print(f'shape of X_train: {X_train.shape}')\n",
    "print(f'shape of X_test: {X_test.shape}')\n",
    "print(f'shape of y_train: {y_train.shape}')\n",
    "print(f'shape of y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd1448",
   "metadata": {},
   "source": [
    "### 5) Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1a1e02-6c19-49d6-9600-025c6ad3c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0e944e-24a1-4073-828f-07c463a0fe34",
   "metadata": {},
   "source": [
    "The number of negative values (normal cases) is way too large compared to the positive class (around 22k positive samples and 1100k negative samples). Then, we are running into an Imbalaned Dataset. It is expected since we are dealing with a predictive maintenance problem.  \n",
    "To address this issue, we ought to balance our data. There are various techniques to balance it. One of them is **Undersampling**. For that, we will be using the `RandomUnderSampler` from `imblearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5156dd3-ff17-4366-a5f9-66144f81bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_balanced, y_train_balanced = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# reassemble the dataset for further preprocessing and EDA\n",
    "balanced_train = X_train_balanced.copy()\n",
    "balanced_train['status'] = y_train_balanced\n",
    "balanced_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75059c6b-7e85-47a0-9845-ae3be23250ee",
   "metadata": {},
   "source": [
    "`balanced_train` is supposed to be balanced now. Let us check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b787c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value counts from the imbalanced dataset\n",
    "imbalanced_class_counts = y_train.value_counts()\n",
    "\n",
    "# value counts from the balanced dataset\n",
    "balanced_class_counts = y_train_balanced.value_counts()\n",
    "\n",
    "# plot pie charts to show the class distribution difference\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].pie(\n",
    "    imbalanced_class_counts,\n",
    "    labels = ['Negative', 'Positive'],\n",
    "    autopct = '%1.1f%%',\n",
    "    startangle = 90,\n",
    "    colors = ['lightpink', 'lightblue']\n",
    ")\n",
    "axes[0].set_title('Before Undersampling')\n",
    "\n",
    "axes[1].pie(\n",
    "    balanced_class_counts,\n",
    "    labels = ['Negative', 'Positive'],\n",
    "    autopct = '%1.1f%%',\n",
    "    startangle = 90,\n",
    "    colors = ['lightpink', 'lightblue']\n",
    ")\n",
    "axes[1].set_title('After Undersampling')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3777688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2d8e03",
   "metadata": {},
   "source": [
    "### 6) Checking for outliers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb42acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outliers(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    num_outliers = len(outliers)\n",
    "    print(f\"Number of outliers in {column}: {num_outliers}\")\n",
    "    return outliers\n",
    "\n",
    "# def remove_outliers(data, column):\n",
    "#     Q1 = data[column].quantile(0.25)\n",
    "#     Q3 = data[column].quantile(0.75)\n",
    "#     IQR = Q3 - Q1\n",
    "#     lower_bound = Q1 - 1.5 * IQR\n",
    "#     upper_bound = Q3 + 1.5 * IQR\n",
    "#     outliers_removed = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n",
    "#     num_removed = len(data) - len(outliers_removed)\n",
    "#     print(f\"Number of outliers removed from {column}: {num_removed}\\n\")\n",
    "#     return outliers_removed\n",
    "\n",
    "# First, identify outliers\n",
    "for col in balanced_train:\n",
    "    if col not in ['timestamp', 'status']:\n",
    "        outliers = identify_outliers(balanced_train, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e786b310",
   "metadata": {},
   "source": [
    "the features: ['COMP', 'DV_eletric','Towers', 'MPG','LPS','Pressure_switch','Oil_level','Caudal_impulses'] are binary features. So we do not remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4587e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the columns with the binary values\n",
    "binary_cols = ['LPS', 'Pressure_switch', 'Oil_level', 'Caudal_impulses']\n",
    "# Ensure the the binary data is binary\n",
    "balanced_train[binary_cols] = balanced_train[binary_cols].apply(np.round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356346ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of unique values in each column\n",
    "for col in balanced_train.columns:\n",
    "    print(f\"number of unique values in {col}: {balanced_train[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd56659",
   "metadata": {},
   "source": [
    "# 2. Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3b218a",
   "metadata": {},
   "source": [
    "### 1) Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation \n",
    "correlation = balanced_train.corr()\n",
    "plt.figure(figsize = (10, 10))\n",
    "sns.heatmap(correlation, annot = True, cmap = 'coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe5a44",
   "metadata": {},
   "source": [
    "From the above correlation heatmap,  we can see that our target feature **\"status\"** has a strong correlation with these features: TP2, H1, DV_pressure, Oil_temparature, Motor_current, COMP, DV_electric and MPG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b845db8",
   "metadata": {},
   "source": [
    "### 2) Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5724c60",
   "metadata": {},
   "source": [
    "1. Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44513f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize all the features outliers in one plot \n",
    "sns.set(rc={'figure.figsize':(20,8.27)})\n",
    "sns.boxplot(data = balanced_train.drop(['timestamp', 'status'], axis = 1))\n",
    "# plt.xticks(rotation = 45)\n",
    "plt.title('Boxplot of all features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee1ba6",
   "metadata": {},
   "source": [
    "2. Probability distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0acb70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module='seaborn')\n",
    "\n",
    "#visualize the probability distribution of all the features\n",
    "def plot_col_distribution(data):\n",
    "    fig, axes = plt.subplots(4, 4, figsize = (20, 10))\n",
    "    axes = axes.flatten()\n",
    "    for i, col in enumerate(data.columns):\n",
    "        data[col] = data[col].replace([np.inf, -np.inf], np.nan)\n",
    "        sns.histplot(data[col], ax = axes[i], kde=True)\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_col_distribution(balanced_train.drop(['timestamp', 'status'], axis = 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b83150",
   "metadata": {},
   "source": [
    "3. Time series plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35580505",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_train.iloc[:,:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec9963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorganize according to timestamp \n",
    "balanced_train.sort_values('timestamp', inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb99ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series\n",
    "balanced_train.iloc[:,:16].plot(\n",
    "        subplots =True,\n",
    "        layout=(6, 3),\n",
    "        figsize=(22,22),\n",
    "        fontsize=10, \n",
    "        linewidth=1,\n",
    "        sharex = False, \n",
    "        title='Visualization of the Original Time Series')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767d78cf-7d8a-44d9-9038-8c9e0202ffcf",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97535f8e-74ff-45e2-bb60-92ae902a4a4e",
   "metadata": {},
   "source": [
    "### 1) Taking out the Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e376c7-68c7-4e17-9f39-4a362f35831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_balanced = X_train_balanced.drop(columns=['timestamp'])\n",
    "X_test = X_test.drop(columns=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2014b265-a530-47a8-9310-8a3e6eb9580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_balanced.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea2e3cc-bf6e-429d-8a4d-58b8f5e69399",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb43c5e-878e-46c0-8fa9-8ef426b25ae5",
   "metadata": {},
   "source": [
    "### 2) Evaluation Protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e10ff2-e055-4e90-b86d-e236d937a73e",
   "metadata": {},
   "source": [
    "Reminder that the aim of this study is to predict failures and the need of maintenance in an\n",
    "urban metro public transportation service. To assess the fit and how good each model performed, we will evaluate it using the following metrics:\n",
    "* **Accuracy:** measures the overall correctness of the model since our data is balanced:\n",
    "\n",
    "$$ Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "\n",
    "\n",
    "* **Precision:** the goal of it is to maximize the positive (failure) predictions when they were originally failures (TP) and minimize false calls (FP) which are non-failure values predicted as failure by the model:\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "* **F1 Score**: harmonic mean of Precision and Recall:\n",
    "\n",
    "$$ F1 Score = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} $$\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d8320c-4455-4dfb-94c5-30321017b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4d7cb0-4cfa-422b-a69b-83dd3c526abd",
   "metadata": {},
   "source": [
    "We maintain a dataframe `scores` to hold the scores of each model we will study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bed47f-5bec-4ed0-9427-db5dee9d8c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(columns=['model', 'accuracy', 'precision', 'f1'])\n",
    "scores # should be empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56743e17-6e81-4874-beac-64df61126971",
   "metadata": {},
   "source": [
    "### 3) Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e815f9-d4d6-439d-8fde-9c664decfab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "dt_preds = dt.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "train_accuracy_dt = accuracy_score(dt.predict(X_train_balanced), y_train_balanced)\n",
    "test_accuracy_dt = accuracy_score(dt_preds, y_test)\n",
    "\n",
    "# precision\n",
    "train_precision_dt = precision_score(dt.predict(X_train_balanced), y_train_balanced, average='macro')\n",
    "test_precision_dt = precision_score(dt_preds, y_test, average='macro')\n",
    "\n",
    "# f1 score\n",
    "train_f1_dt = f1_score(dt.predict(X_train_balanced), y_train_balanced, average='macro')\n",
    "test_f1_dt = f1_score(dt_preds, y_test, average='macro')\n",
    "\n",
    "print(f'accuracy:')\n",
    "print(f'  train: {train_accuracy_dt}')\n",
    "print(f'  test: {test_accuracy_dt}')\n",
    "\n",
    "print(f'\\nprecision:')\n",
    "print(f'  train: {train_precision_dt}')\n",
    "print(f'  test: {test_precision_dt}')\n",
    "\n",
    "print(f'\\nf1:')\n",
    "print(f'  train: {train_f1_dt}')\n",
    "print(f'  test: {test_f1_dt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a5d6c7-8ec1-4516-8c3f-29f39045005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to scores dataframe\n",
    "scores.loc[len(scores)] = ['Decision Tree', test_accuracy_dt, test_precision_dt, test_f1_dt]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0abdc-b426-4526-ab6a-4476968f4ab7",
   "metadata": {},
   "source": [
    "### 4) Hyperparameterized Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03596e4-4854-4b98-b4cc-7123c48a7995",
   "metadata": {},
   "source": [
    "Here, we are going to find the best Decision Tree Classifier. That is, we aim to find its parameters that best maximize the score. This is done by **Random Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7e09ff-7fe9-4a00-aef6-bc8ba4c63b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "dt_params = {\n",
    "    'criterion': ['gini'],\n",
    "    'max_depth': randint(1, 50),\n",
    "    'min_samples_split': randint(2, 11),\n",
    "    'min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search_dt = RandomizedSearchCV(\n",
    "    estimator=dt,\n",
    "    param_distributions=dt_params,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    n_jobs=1,\n",
    "    random_state=42,\n",
    "    scoring='f1_macro'\n",
    ")\n",
    "\n",
    "random_search_dt.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "best_params_dt = random_search_dt.best_params_\n",
    "best_score_dt = random_search_dt.best_score_\n",
    "\n",
    "print(f'best decision tree parameters: {best_params_dt}')\n",
    "print(f'best score (F1): {best_score_dt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e87540-abb9-4093-b977-257d943cef0f",
   "metadata": {},
   "source": [
    "Now, we model with teh best **Decision Tree** best on the **Random Search**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e64d0ca-25de-4a95-ae8d-9733d2d73e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dt = random_search_dt.best_estimator_\n",
    "best_dt.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "best_dt_preds = best_dt.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "train_accuracy_best_dt = accuracy_score(best_dt.predict(X_train_balanced), y_train_balanced)\n",
    "test_accuracy_best_dt = accuracy_score(best_dt_preds, y_test)\n",
    "\n",
    "# precision\n",
    "train_precision_best_dt = precision_score(best_dt.predict(X_train_balanced), y_train_balanced, average='macro')\n",
    "test_precision_best_dt = precision_score(best_dt_preds, y_test, average='macro')\n",
    "\n",
    "# f1 score\n",
    "train_f1_best_dt = f1_score(best_dt.predict(X_train_balanced), y_train_balanced, average='macro')\n",
    "test_f1_best_dt = f1_score(best_dt_preds, y_test, average='macro')\n",
    "\n",
    "print(f'accuracy:')\n",
    "print(f'  train: {train_accuracy_best_dt}')\n",
    "print(f'  test: {test_accuracy_best_dt}')\n",
    "\n",
    "print(f'\\nprecision:')\n",
    "print(f'  train: {train_precision_best_dt}')\n",
    "print(f'  test: {test_precision_best_dt}')\n",
    "\n",
    "print(f'\\nf1:')\n",
    "print(f'  train: {train_f1_best_dt}')\n",
    "print(f'  test: {test_f1_best_dt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1db338-6102-4ab0-9285-d994f8fbc5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to scores dataframe\n",
    "scores.loc[len(scores)] = ['Hyperparametirized Decision Tree', test_accuracy_best_dt, test_precision_best_dt, test_f1_best_dt]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621df7f7-ec02-4baf-bb8d-41f214884dd1",
   "metadata": {},
   "source": [
    "### 5) Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3eab44-d260-4799-9e16-b9a141873a40",
   "metadata": {},
   "source": [
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. Trees in the forest use the best split strategy, i.e. equivalent to passing `splitter=\"best\"` to the underlying DecisionTreeRegressor. The sub-sample size is controlled with the `max_samples` parameter if `bootstrap=True` (default), otherwise the whole dataset is used to build each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be76daf-a2ea-4017-9117-e0f415c66f7a",
   "metadata": {},
   "source": [
    "**building a failure prediction model**\n",
    "\n",
    "The goal is to employ the random forest classifier algorithm in order to predict failure labeled as 0 and 1 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827d0281-2772-4264-8200-666dce538a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "rf_preds = rf.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "train_accuracy_rf = accuracy_score(rf.predict(X_train_balanced), y_train_balanced)\n",
    "test_accuracy_rf = accuracy_score(rf_preds, y_test)\n",
    "\n",
    "# precision\n",
    "train_precision_rf = precision_score(rf.predict(X_train_balanced), y_train_balanced)\n",
    "test_precision_rf = precision_score(rf_preds, y_test)\n",
    "\n",
    "# f1 score\n",
    "train_f1_rf = f1_score(rf.predict(X_train_balanced), y_train_balanced)\n",
    "test_f1_rf = f1_score(rf_preds, y_test)\n",
    "\n",
    "print(f'accuracy:')\n",
    "print(f'train: {train_accuracy_rf}')\n",
    "print(f'test: {test_accuracy_rf}')\n",
    "\n",
    "print(f'\\nprecision:')\n",
    "print(f'train: {train_precision_rf}')\n",
    "print(f'test: {test_precision_rf}')\n",
    "\n",
    "print(f'\\nf1:')\n",
    "print(f'train : {train_f1_rf}')\n",
    "print(f'test: {test_f1_rf}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7be98-98cb-4307-80fa-63ad51803226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "print(classification_report(y_test, rf_preds))\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_test, rf_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf.classes_)\n",
    "disp.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64706f2c-df5e-4e33-bbda-be7a972cd380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to scores dataframe\n",
    "scores_rf = {\n",
    "    'accuracy': test_accuracy_rf,\n",
    "    'precision': test_precision_rf,\n",
    "    'f1': test_f1_rf\n",
    "}\n",
    "\n",
    "scores_rf_df = pd.DataFrame(scores_rf, index=[0])\n",
    "scores.loc[len(scores)] = ['Random Forest', test_accuracy_rf, test_precision_rf, test_f1_rf]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21f63c7-9fd2-466d-b419-ecda8b67aa1e",
   "metadata": {},
   "source": [
    "**hyperparameters of Random Forest** \n",
    "\n",
    "Random forest has several hyperparameters that influence its performance: \n",
    "- `n_estimators`: The number of trees in the forest. Increasing this value generally improves performance until a certain point\n",
    "\n",
    "- ` max_depth`: The maximum depth of each tree. Deeper trees may capture more complex relationships but this may lead to an overfitting\n",
    "\n",
    "- `min_samples_split`: The minimum number of samples required to split an internal node.\n",
    "\n",
    "- `min_samples_leaf`:  The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least `min_samples_leaf` training samples in each of the left and right branches. This may have the effect of smoothing the model\n",
    "\n",
    "-  `max_features`: The maximum number of features to consider when looking for the best split.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ed9cda-cb86-4c72-968f-4d343137fe2d",
   "metadata": {},
   "source": [
    "We will fine-tune the model to achieve the most optimal performance by adjusting these hyperparametrs. Grid search and Cross-Validation common techniques for finding the best combination of hyperparameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789d65a4-d229-4fdb-8e38-81fcd43292c6",
   "metadata": {},
   "source": [
    "**Random search** is another powerful technique for optimizing the hyperparameters of a model. It works in a similar way to grid search cross-validation (GridSearchCV), but instead of searching over a predefined grid of hyperparameters, it samples them randomly from a distribution,  without becoming too computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a515a86c-fe9e-4bda-974d-4f5b4fc5f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized Search CV\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "\n",
    "}\n",
    "\n",
    "# Perform Randomized Search CV\n",
    "Random_search = RandomizedSearchCV(rf, param_dist, cv=5, n_iter=10, n_jobs=-1, random_state=42)\n",
    "Random_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Extract the cross-validation results and hyperparameters\n",
    "cv_results = Random_search.cv_results_['mean_test_score']\n",
    "best_params = Random_search.cv_results_['params']\n",
    "\n",
    "# Get the best params \n",
    "best_params = Random_search.best_params_\n",
    "\n",
    "# train the model with the best params\n",
    "rf_best = Random_search.best_estimator_\n",
    "rf_best.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best Hyperparameters:')\n",
    "print(best_params)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_rf = rf_best.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "precision = precision_score(y_test, y_pred_rf)\n",
    "f1 = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"Random Forest Classifier:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Display confusion matrix\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "disp_forest = ConfusionMatrixDisplay(conf_matrix_rf, display_labels=rf_best.classes_)\n",
    "disp_forest.plot(cmap='Blues', values_format='d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43171209-23c5-42e3-b434-438cec1a5951",
   "metadata": {},
   "source": [
    "- **Best Hyperparameters:**\n",
    "\n",
    "    {'n_estimators': 100, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed03240-a984-413c-9de2-b614fd9a89f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to scores dataframe\n",
    "scores_rf_best = {\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'f1': f1\n",
    "}\n",
    "\n",
    "scores_rf_best_df = pd.DataFrame(scores_rf_best, index=[0])\n",
    "scores.loc[len(scores)] = ['Random Forest (Best)', accuracy, precision, f1]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dc8ec4-92a6-4f8c-b75a-4cd4af5eb998",
   "metadata": {},
   "source": [
    "**Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbda4af-767e-4030-af0f-b411b106c592",
   "metadata": {},
   "source": [
    "From the Confusion matrix, we observe that the number of false positive and false negative has slightly decreased when using RandomSearch and hyperparamater tuning: \n",
    "- class 0: 20 -> 19\n",
    "- class 1: 23 -> 21\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e25abd2-1dcc-4802-9431-668528a5bbc5",
   "metadata": {},
   "source": [
    "### 6) K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b5f93b-0061-4739-87e2-b951f67ea1a6",
   "metadata": {},
   "source": [
    "### 7) Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0801c3e-7e62-46ee-8c96-f749a5bb8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_balanced, y_train_balanced)\n",
    "gnb_preds = gnb.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "train_accuracy_gnb = accuracy_score(gnb.predict(X_train_balanced), y_train_balanced)\n",
    "test_accuracy_gnb = accuracy_score(gnb_preds, y_test)\n",
    "\n",
    "# precision\n",
    "train_precision_gnb = precision_score(gnb.predict(X_train_balanced), y_train_balanced, average='macro')\n",
    "test_precision_gnb = precision_score(gnb_preds, y_test, average='macro')\n",
    "\n",
    "# f1 score\n",
    "train_f1_gnb = f1_score(gnb.predict(X_train_balanced), y_train_balanced, average='macro')\n",
    "test_f1_gnb = f1_score(gnb_preds, y_test, average='macro')\n",
    "\n",
    "print(f'accuracy:')\n",
    "print(f'  train: {train_accuracy_gnb}')\n",
    "print(f'  test: {test_accuracy_gnb}')\n",
    "\n",
    "print(f'\\nprecision:')\n",
    "print(f'  train: {train_precision_gnb}')\n",
    "print(f'  test: {test_precision_gnb}')\n",
    "\n",
    "print(f'\\nf1:')\n",
    "print(f'  train: {train_f1_gnb}')\n",
    "print(f'  test: {test_f1_gnb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5d97a-6c07-4db7-a1d8-73601cea5137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to scores dataframe\n",
    "scores.loc[len(scores)] = ['Gaussian Naive Bayes', test_accuracy_gnb, test_precision_gnb, test_f1_gnb]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc4733-d7d0-4063-84a4-ac674af6041a",
   "metadata": {},
   "source": [
    "As noticeable, Gaussian Naive Bayes did perform quite well. Yet, worse than previous classifiers. This is due to the nature of the features, half of them follow a similar to Normal distribution and the rest follow a Bernoulli distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69997a4e-f10a-4a03-87f3-1d3149bab329",
   "metadata": {},
   "source": [
    "### 8) Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd0b95c-73b4-4f67-a9be-abcdf50daa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='linear', degree=3)\n",
    "svm.fit(X_train_balanced, y_train_balanced)\n",
    "svm_preds = svm.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "train_accuracy_svm = accuracy_score(svm.predict(X_train_balanced), y_train_balanced)\n",
    "test_accuracy_svm = accuracy_score(svm_preds, y_test)\n",
    "\n",
    "# precision\n",
    "train_precision_svm = precision_score(svm.predict(X_train_balanced), y_train_balanced, average='macro')\n",
    "test_precision_svm = precision_score(svm_preds, y_test, average='macro')\n",
    "\n",
    "# f1 score\n",
    "train_f1_svm = f1_score(svm.predict(X_train_balanced), y_train_balanced, average='macro')\n",
    "test_f1_svm = f1_score(svm_preds, y_test, average='macro')\n",
    "\n",
    "print(f'accuracy:')\n",
    "print(f'  train: {train_accuracy_svm}')\n",
    "print(f'  test: {test_accuracy_svm}')\n",
    "\n",
    "print(f'\\nprecision:')\n",
    "print(f'  train: {train_precision_svm}')\n",
    "print(f'  test: {test_precision_svm}')\n",
    "\n",
    "print(f'\\nf1:')\n",
    "print(f'  train: {train_f1_svm}')\n",
    "print(f'  test: {test_f1_svm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b64abf9-0824-4637-baff-8f7334d1933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to scores dataframe\n",
    "scores.loc[len(scores)] = ['SVM', test_accuracy_svm, test_precision_svm, test_f1_svm]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2491cf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "muo env",
   "language": "python",
   "name": "muo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
